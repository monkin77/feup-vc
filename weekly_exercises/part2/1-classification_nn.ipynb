{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaU9_sOGzRD1"
      },
      "source": [
        "# Neural Networks\n",
        "In this notebook we will learn how to train a simple Multilayer Perceptron for image classification using PyTorch. You can find additional information [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_o30wUG8zRD7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\python38\\lib\\site-packages (2.0.0)\n",
            "Requirement already satisfied: jinja2 in c:\\python38\\lib\\site-packages (from torch) (3.0.1)\n",
            "Requirement already satisfied: networkx in c:\\python38\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: sympy in c:\\python38\\lib\\site-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python38\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\python38\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\python38\\lib\\site-packages (0.15.1)\n",
            "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from torchvision) (1.21.0)\n",
            "Requirement already satisfied: torch==2.0.0 in c:\\python38\\lib\\site-packages (from torchvision) (2.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python38\\lib\\site-packages (from torchvision) (9.1.1)\n",
            "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from torchvision) (2.26.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch==2.0.0->torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: networkx in c:\\python38\\lib\\site-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: sympy in c:\\python38\\lib\\site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in c:\\python38\\lib\\site-packages (from torch==2.0.0->torchvision) (3.0.1)\n",
            "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from torch==2.0.0->torchvision) (3.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->torchvision) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->torchvision) (1.26.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python38\\lib\\site-packages (from jinja2->torch==2.0.0->torchvision) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\python38\\lib\\site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.2-cp38-cp38-win_amd64.whl (8.3 MB)\n",
            "     ---------------------------------------- 8.3/8.3 MB 4.6 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "     -------------------------------------- 298.0/298.0 kB 6.3 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
            "     ---------------------------------------- 42.2/42.2 MB 7.1 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\python38\\lib\\site-packages (from scikit-learn) (1.21.0)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\joaog\\appdata\\roaming\\python\\python38\\site-packages (from tqdm) (0.4.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dvfTDUXuzRD9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDBhCwhszREA"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w5IQVCmOzREB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:07<00:00, 1276867.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 75615.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1259512.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3172444.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# torchvision has some datasets already included, so we will load MNIST through torchvision\n",
        "# first we need to define the transformations\n",
        "\n",
        "data_aug = transforms.Compose([transforms.ToTensor()]) # the ToTensor transform scales the image into [0., 1.0] range\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "validation_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "indices = list(range(len(validation_data)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_size = 0.2 * len(indices)\n",
        "split = int(np.floor(test_size))\n",
        "val_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ByDvrH3z1uOT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "5\n",
            "torch.Size([64, 1, 28, 28])\n",
            "tensor([2, 3, 2, 9, 8, 7, 3, 2, 1, 3, 1, 0, 7, 3, 3, 4, 5, 8, 3, 3, 1, 0, 2, 6,\n",
            "        0, 5, 1, 5, 4, 6, 1, 7, 6, 9, 9, 0, 6, 5, 6, 2, 7, 9, 3, 3, 8, 0, 3, 2,\n",
            "        1, 0, 2, 1, 0, 7, 4, 6, 5, 1, 7, 5, 0, 1, 0, 3])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# now we need to define a Dataloader, which allows us to automatically batch our inputs, do sampling and multiprocess data loading\n",
        "batch_size = 64\n",
        "num_workers = 2 # how many processes are used to load the data\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "test_dataloader = DataLoader(validation_data, sampler=test_sampler, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "# let's visualize the data\n",
        "# alternative 1: using the Dataset\n",
        "sample = training_data[0] \n",
        "img = sample[0]\n",
        "label = sample[1]\n",
        "print(img.shape) # note that here we only get one image and its label\n",
        "print(label)\n",
        "\n",
        "# alternative 2: iterate over the Dataloader\n",
        "for batch in train_dataloader:\n",
        "  imgs = batch[0]\n",
        "  labels = batch[1]\n",
        "  print(imgs.shape)\n",
        "  print(labels)\n",
        "\n",
        "  plt.imshow(imgs[0][0,:,:], cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aqWR_0VzRED"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "Create an MLP with the following structure: \n",
        "\n",
        "1. Dense/linear layer that takes the images as a flattened input vector and generates an output of 512 of dimension.\n",
        "2. ReLU activation layer\n",
        "3. Dense/linear layer with 512 input and output\n",
        "3. ReLU activation layer\n",
        "4. Dense/linear layer with 10 output channels (10 classes of MNIST)\n",
        "\n",
        "You can use PyTorch's layers: https://pytorch.org/docs/stable/nn.html (Conv2d, ReLU, Linear, MaxPool2d, Dropout, Flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfK3c9RSzRED"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # TODO \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "         # TODO \n",
        "\n",
        "model = NeuralNetwork().to(device) # put model in device (GPU or CPU)\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpret the implemented architecture and try to answer the following questions:\n",
        "\n",
        "a) What is the shape (width, and # of channels) of the output tensor after the first layer?\n",
        "\n",
        "b) And after the first 3 layers (dense+dense+dense)?\n",
        "\n",
        "c) How many parameters (weights) does the model have? Contrary to Keras, PyTorch does not have an official method for counting the number of parameters of a model, but you can use [torchsummary](https://pypi.org/project/torch-summary/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install torch-summary\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g1kRGiw_zREE"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define:\n",
        "1. The loss function\n",
        "2. The optimiser \n",
        "3. The training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# loss_fn = ...\n",
        "# optimizer = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxSdayviCWk5"
      },
      "outputs": [],
      "source": [
        "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
        "    if is_train:\n",
        "      assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
        "      \n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    if is_train:\n",
        "      model.train() # put model in train mode\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # TODO: Compute prediction error\n",
        "          # pred = ...\n",
        "          # loss = ...\n",
        "\n",
        "          if is_train:\n",
        "            # TODO: Backpropagation\n",
        "\n",
        "          # TODO: Compute final prediction (softmax + argmax)\n",
        "          # probs = ...\n",
        "          # final_pred = ...\n",
        "\n",
        "          # Save training metrics\n",
        "          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n",
        "\n",
        "          preds.extend(final_pred.cpu().numpy())\n",
        "          labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / num_batches, accuracy_score(labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmsUVGS6C0O1"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "train_history = {'loss': [], 'accuracy': []}\n",
        "val_history = {'loss': [], 'accuracy': []}\n",
        "best_val_loss = np.inf\n",
        "print(\"Start training...\")\n",
        "for t in range(num_epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
        "    print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n",
        "    val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n",
        "    print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n",
        "\n",
        "    # save model when val loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "      torch.save(save_dict, 'best_model.pth')\n",
        "\n",
        "    # save latest model\n",
        "    save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "    torch.save(save_dict, 'latest_model.pth')\n",
        "\n",
        "    # save training history for plotting purposes\n",
        "    train_history[\"loss\"].append(train_loss)\n",
        "    train_history[\"accuracy\"].append(train_acc)\n",
        "\n",
        "    val_history[\"loss\"].append(val_loss)\n",
        "    val_history[\"accuracy\"].append(val_acc)\n",
        "    \n",
        "print(\"Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQMAKFHzREG"
      },
      "source": [
        "## Analyse training evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr48TEVlzREH"
      },
      "outputs": [],
      "source": [
        "def plotTrainingHistory(train_history, val_history):\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(train_history['loss'], label='train')\n",
        "    plt.plot(val_history['loss'], label='val')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(train_history['accuracy'], label='train')\n",
        "    plt.plot(val_history['accuracy'], label='val')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfeNPc4zREI"
      },
      "outputs": [],
      "source": [
        "plotTrainingHistory(train_history, val_history)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HPZLw5cfzREI"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "Evaluate the model in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtmFHipizREK"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1yyEC9pzREJ"
      },
      "outputs": [],
      "source": [
        "def showErrors(model, dataloader, num_examples=20):    \n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    for ind, (X, y) in enumerate(dataloader):\n",
        "      if ind >= 20: break\n",
        "      X, y = X.to(device), y.to(device)    \n",
        "      pred = model(X)\n",
        "      probs = F.softmax(pred, dim=1)\n",
        "      final_pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "      plt.subplot(10, 10, ind + 1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.text(0, -1, y[0].item(), fontsize=14, color='green') # correct\n",
        "      plt.text(8, -1, final_pred[0].item(), fontsize=14, color='red')  # predicted\n",
        "      plt.imshow(X[0][0,:,:].cpu(), cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh38evzTzREL"
      },
      "outputs": [],
      "source": [
        "showErrors(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-C6glKseuO"
      },
      "source": [
        "## Additional Challenges\n",
        "\n",
        "a) As the test accuracy should show, the MNIST dataset is not very challenging, change the code to use Fashion-MNIST and compare the results.\n",
        "\n",
        "b) Do the same for the CIFAR10 (or CIFAR100) dataset. Note that, in this case, each image is a 32x32 color image; convert it to grayscale or concatenate the RGB channels in one single vector (e.g. using the reshape method).\n",
        "\n",
        "c) The test accuracy for CIFAR is significantly worse. Try improving the results by using: 1) a deeper architecture, and 2) a different optmizer.\n",
        "\n",
        "You can load the datasets from [here](https://pytorch.org/vision/stable/datasets.html).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nn_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
