{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Wally Tutorial\n",
    "\n",
    "In this exercise we are going to play the \"Find wally\" game. \n",
    "\n",
    "- **Inputs**: \n",
    "    You have 2 images, one with the Wally's profile, and another with the puzzle, were we should find Wally.\n",
    "\n",
    "- **Expected result**:\n",
    "    Visual location of Wally in the provided puzzle. \n",
    "\n",
    "## Requirements\n",
    "You'll need the following libraries: \n",
    "- `ipykernel`\n",
    "- `matplotlib`\n",
    "- `opencv-python`\n",
    "- `opencv-contrib-python`\n",
    "- `nb_black`\n",
    "- `ipympl`\n",
    "- `scipy`\n",
    "- `scikit-image`\n",
    "\n",
    "## Guides\n",
    "In the previous class, we introduced Local Features. \n",
    "This exercise is a great example of the one application of the local features, especially doe to its mental complexity. \n",
    "\n",
    "An option to implement this mini project is to: \n",
    "1. Find keypoints in both provided images\n",
    "2. Build their descriptors\n",
    "3. Match the descriptors between the 2 images\n",
    "4. Run outlier removal algorithms to remove the incorrect matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the input images\n",
    "\n",
    "Steps: \n",
    "1. Read the two images\n",
    "2. Convert them to RGB\n",
    "3. Have a version of them in GrayScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img_wallys_face = cv2.imread(\"wally.png\", 0)\n",
    "img_find_wally = cv2.imread(\"find_wally.jpeg\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE, TO PREPARE THE IMAGE ---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].set_title(\"Wally's Face\")\n",
    "ax[0].imshow(img_wallys_face_gray, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].set_title(\"Find Wally\")\n",
    "ax[1].imshow(img_find_wally_gray, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect keypoints and create descriptor\n",
    "\n",
    "During the class we mentioned SIFT, however you can use any algorithm for detection and description.\n",
    "\n",
    "Steps: \n",
    "1. Find keypoints from the 2 grayscale images\n",
    "2. Store their descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE, TO PREPARE DETECT THE KEYPOINTS, AND RESPECTIVE DESCRIPTORS ---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw keypoints in pattern's image - Wally's image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wally's face\n",
    "keypoints_with_size = np.copy(img_wallys_face_rgb)\n",
    "\n",
    "cv2.drawKeypoints(\n",
    "    img_wallys_face_gray,\n",
    "    pattern_keypoints,\n",
    "    keypoints_with_size,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    ")\n",
    "\n",
    "# Find Wally board\n",
    "keypoints_with_size_find_wally = np.copy(img_find_wally_rgb)\n",
    "\n",
    "cv2.drawKeypoints(\n",
    "    img_find_wally_gray,\n",
    "    to_find_keypoints,\n",
    "    keypoints_with_size_find_wally,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    ")\n",
    "\n",
    "# Print\n",
    "fx, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].set_title(\"Pattern keypoints With Size\")\n",
    "ax[0].imshow(keypoints_with_size, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].set_title(\"Find Wally keypoints With Size\")\n",
    "ax[1].imshow(keypoints_with_size_find_wally, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the wally image\n",
    "print(\"Number of Keypoints Detected In Wally's Image: \", len(pattern_keypoints))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the descriptors between the 2 images - find correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE, TO PREPARE MATCH THE DESCRIPTORS BETWEEN THE 2 IMAGES ---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.drawMatches(\n",
    "    img_wallys_face_rgb,\n",
    "    pattern_keypoints,\n",
    "    img_find_wally_rgb,\n",
    "    to_find_keypoints,\n",
    "    matches,\n",
    "    img_find_wally_gray,\n",
    "    flags=2,\n",
    ")\n",
    "\n",
    "# Display the best matching points\n",
    "plt.rcParams[\"figure.figsize\"] = [14.0, 7.0]\n",
    "plt.title(\"Best Matching Points\")\n",
    "plt.imshow(result)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Print total number of matching points between Wally and the puzzle board\n",
    "print(\n",
    "    \"\\nNumber of Matching Keypoints Between The Training and Query Images: \",\n",
    "    len(matches),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a lot of matches are incorrect. So we need a way to remove the outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the incorrect matches / outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE, REMOVE THE INCORRECT MATCHES / OUTLIERS ---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_wally_found = cv2.drawMatches(\n",
    "    img_wallys_face_rgb,\n",
    "    inlier_keypoints_left,\n",
    "    img_find_wally_rgb,\n",
    "    inlier_keypoints_right,\n",
    "    placeholder_matches,\n",
    "    None,\n",
    ")\n",
    "\n",
    "# Display the best matching points\n",
    "plt.rcParams[\"figure.figsize\"] = [14.0, 7.0]\n",
    "plt.title(\"After RANSAC\")\n",
    "plt.imshow(img_wally_found)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
